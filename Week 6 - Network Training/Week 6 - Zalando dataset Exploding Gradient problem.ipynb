{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/michelucci/zhaw-dlcourse-spring2019/blob/master/Week%206%20-%20Network%20Training/Week%206%20-%20Zalando%20dataset%20and%20decaying%20learning%20rate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uydiM6DeAF7F"
   },
   "source": [
    "# Neural Networks and Deep Learning for Life Sciences and Health Applications - An introductory course about theoretical fundamentals, case studies and implementations in python and tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTvFYUvhAF7I"
   },
   "source": [
    "(C) Umberto Michelucci 2018 - umberto.michelucci@gmail.com \n",
    "\n",
    "github repository: https://github.com/michelucci/zhaw-dlcourse-spring2019\n",
    "\n",
    "Spring Semester 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploding Gradient problem\n",
    "\n",
    "In this notebook we will discuss the problem of exploding gradients when using the ReLU activation function, with the help of the fashion MNIST (Zalando) dataset. I will skip the data preparation discussion (check the other notebooks in this folder) and will concentrate on the problem itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nbjC9hPPAF7J"
   },
   "source": [
    "# Load of the data\n",
    "\n",
    "Information can be found here\n",
    "\n",
    "https://www.kaggle.com/zalando-research/fashionmnist/data\n",
    "\n",
    "Note that this notebook expect the files to be in this folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NFtNVSeDAF7J"
   },
   "source": [
    "**Context**\n",
    "Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n",
    "\n",
    "The original MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try. \"If it doesn't work on MNIST, it won't work at all\", they said. \"Well, if it does work on MNIST, it may still fail on others.\"\n",
    "Zalando seeks to replace the original MNIST dataset\n",
    "\n",
    "**Content**\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), and represents the article of clothing. The rest of the columns contain the pixel-values of the associated image.\n",
    "\n",
    "To locate a pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27. The pixel is located on row i and column j of a 28 x 28 matrix. \n",
    "For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below. \n",
    "\n",
    "**Labels**\n",
    "Each training and test example is assigned to one of the following labels:\n",
    "- 0 T-shirt/top\n",
    "- 1 Trouser\n",
    "- 2 Pullover\n",
    "- 3 Dress\n",
    "- 4 Coat\n",
    "- 5 Sandal\n",
    "- 6 Shirt\n",
    "- 7 Sneaker\n",
    "- 8 Bag\n",
    "- 9 Ankle boot \n",
    "\n",
    "**TL;DR**\n",
    "Each row is a separate image \n",
    "Column 1 is the class label. \n",
    "Remaining columns are pixel numbers (784 total). \n",
    "Each value is the darkness of the pixel (1 to 255)\n",
    "\n",
    "**Acknowledgements**\n",
    "Original dataset was downloaded from https://github.com/zalandoresearch/fashion-mnist\n",
    "Dataset was converted to CSV with this script: https://pjreddie.com/projects/mnist-in-csv/\n",
    "\n",
    "**License**\n",
    "The MIT License (MIT) Copyright © [2017] Zalando SE, https://tech.zalando.com\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W7u88QxkAF7L"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aSxQ9tvGAF7T",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('fashion-mnist_train.csv', header = 0)\n",
    "data_test = pd.read_csv('fashion-mnist_test.csv', header = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x4JrbrK_AF7e"
   },
   "source": [
    "# Train dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u8kE1MycAF7e"
   },
   "outputs": [],
   "source": [
    "labels = data_train['label'].values.reshape(1, 60000)\n",
    "\n",
    "labels_ = np.zeros((60000, 10))\n",
    "labels_[np.arange(60000), labels] = 1\n",
    "labels_ = labels_.transpose()\n",
    "\n",
    "\n",
    "train = data_train.drop('label', axis=1).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ygRY3TtjAF7h",
    "outputId": "d335cfac-14ab-4dff-db27-99ee5946504c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60000)\n",
      "(784, 60000)\n"
     ]
    }
   ],
   "source": [
    "print(labels_.shape)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CULnPNpmAF7k"
   },
   "source": [
    "# Test dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0HPNybDKAF7k"
   },
   "outputs": [],
   "source": [
    "labels_test = data_test['label'].values.reshape(1, 10000)\n",
    "\n",
    "labels_test_ = np.zeros((10000, 10))\n",
    "labels_test_[np.arange(10000), labels_test] = 1\n",
    "labels_test_ = labels_test_.transpose()\n",
    "\n",
    "\n",
    "test = data_test.drop('label', axis=1).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pp-Go8OyAF7q"
   },
   "source": [
    "### Normalization of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f3-h_LkvAF7q"
   },
   "source": [
    "Let's normalize the training data dividing by 255.0 to get the values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6zBxjUK9AF7r"
   },
   "outputs": [],
   "source": [
    "train = np.array(train / 255.0)\n",
    "test = np.array(test / 255.0)\n",
    "labels_ = np.array(labels_)\n",
    "labels_test_ = np.array(labels_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem description\n",
    "\n",
    "The problem is the following. If we create a network with one layer and 15 neurons (see below) and we use ReLU as activation functions, when using a learning rate $\\gamma = 0.01$ we will see ```nan``` appears quite soon if not immediately. The reason is that the gradient of the ReLU function can only be 0 or 1. Let's try to understand better the problem and identify it. Let's start with a network with one hidden layer with one neuron and ten output neurons (for multi-class classification) and a softmax activation function.\n",
    "\n",
    "Note that this problem can be reproduced when \n",
    "\n",
    "- using the ```Adam``` optimizer and not the ```GD```.\n",
    "- using a mini-batch of 50\n",
    "- using a learning rate of $\\gamma = 0.01$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X5R-_UPhAF7_"
   },
   "source": [
    "# Network with 1 layer and 15 neurons - with consant learning rate $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hb_EWupPAF8A"
   },
   "outputs": [],
   "source": [
    "n_dim = 784\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tf.random.set_random_seed(42)\n",
    "\n",
    "# Number of neurons in the layers\n",
    "n1 = 15 # Number of neurons in layer 1\n",
    "n2 = 10 # Number of neurons in output layer \n",
    "\n",
    "cost_history = np.empty(shape=[1], dtype = float)\n",
    "learning_rate = tf.placeholder(tf.float64, shape=())\n",
    "\n",
    "st = 1.0/np.sqrt(n_dim)\n",
    "\n",
    "X = tf.placeholder(tf.float64, [n_dim, None])\n",
    "Y = tf.placeholder(tf.float64, [10, None])\n",
    "W1 = tf.Variable(tf.truncated_normal([n1, n_dim], stddev=st, dtype = tf.float64), dtype = tf.float64) \n",
    "b1 = tf.Variable( tf.zeros([n1,1], dtype = tf.float64)) # tf.constant(0.1, shape = [n1,1])\n",
    "W2 = tf.Variable(tf.truncated_normal([n2, n1], stddev=st, dtype = tf.float64)) \n",
    "b2 = tf.Variable(tf.zeros([n2,1], dtype = tf.float64)) \n",
    "                 \n",
    "    \n",
    "ZZ = tf.matmul(W1, X) + b1\n",
    "# Let's build our network...\n",
    "Z1 = tf.nn.relu(tf.matmul(W1, X) + b1) # n1 x n_dim * n_dim x n_obs = n1 x n_obs\n",
    "Z2 = tf.matmul(W2, Z1) + b2 # n2 x n1 * n1 * n_obs = n2 x n_obs\n",
    "y_ = tf.nn.softmax(Z2,0) # n2 x n_obs (10 x None)\n",
    "\n",
    "cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O6q0YzryAF8C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached epoch 0 cost J = 0.32545125752288273\n",
      "Reached epoch 10 cost J = 0.3248716516422052\n",
      "Reached epoch 20 cost J = 0.324286508719163\n",
      "Reached epoch 30 cost J = 0.32364431586960424\n",
      "Reached epoch 40 cost J = 0.32291914191888776\n",
      "Reached epoch 50 cost J = 0.3220928779765425\n",
      "Reached epoch 60 cost J = 0.3211361317420825\n",
      "Reached epoch 70 cost J = 0.3200077123419853\n",
      "Reached epoch 80 cost J = 0.31866034846478775\n",
      "Reached epoch 90 cost J = 0.31704463973505254\n",
      "Reached epoch 100 cost J = 0.31510980542814526\n",
      "CPU times: user 4min 2s, sys: 2min 1s, total: 6min 4s\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "    \n",
    "minibatch = 50\n",
    "    \n",
    "cost_history = []\n",
    "for epoch in range(100+1):\n",
    "    for i in range(0, train.shape[1], minibatch):\n",
    "        X_train_mini = train[:,i:i + minibatch]\n",
    "        y_train_mini = labels_[:,i:i + minibatch]\n",
    "\n",
    "        sess.run(optimizer, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: 1e-4})\n",
    "    cost_ = sess.run(cost, feed_dict={ X:train, Y: labels_, learning_rate: 1e-4})\n",
    "    cost_history = np.append(cost_history, cost_)\n",
    "\n",
    "    if (epoch % 10 == 0):\n",
    "        print(\"Reached epoch\",epoch,\"cost J =\", cost_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Y0aFzbzAF8D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.29365\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
    "print (\"Accuracy:\", accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7iAH3CeEAF8F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2904\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
    "print (\"Accuracy:\", accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tbC4O79EAF8I"
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Week 6 - Zalando dataset and decaying learning rate.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
